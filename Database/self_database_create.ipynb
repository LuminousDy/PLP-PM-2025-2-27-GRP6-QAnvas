{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\common\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Anaconda\\envs\\common\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MongoDB connection\n",
    "mongo_client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = mongo_client[\"canvas_qa_system\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collections for different data types\n",
    "course_collection = db[\"courses\"]\n",
    "file_collection = db[\"files\"] \n",
    "assignment_collection = db[\"assignments\"]\n",
    "announcement_collection = db[\"announcements\"]\n",
    "query_log_collection = db[\"query_logs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WIN11\\AppData\\Local\\Temp\\ipykernel_11552\\1186902046.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "C:\\Users\\WIN11\\AppData\\Local\\Temp\\ipykernel_11552\\1186902046.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB for vector storage\n",
    "CHROMA_PATH = \"./chroma_db\"\n",
    "model_name = 'Snowflake/snowflake-arctic-embed-l-v2.0'\n",
    "model = SentenceTransformer(model_name)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs={'prompt_name': 'query'} # Add prompt_name for queries\n",
    ")\n",
    "vector_db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "BASE_URL = \"https://canvas.nus.edu.sg/api/v1\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {os.getenv('DINGYI_CANVAS_API_KEY')}\"}\n",
    "PAGE_SIZE = 100\n",
    "RATE_LIMIT_DELAY = 0.1  # Delay between API calls to avoid rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_paginated_results(url: str) -> List[Dict[Any, Any]] or None:\n",
    "    \"\"\"\n",
    "    Generic function to get paginated results from Canvas API.\n",
    "    - For announcements (using discussion_topics endpoint with only_announcements param), returns empty list on 404\n",
    "    - Returns None on 403 permission denied to stop crawling that resource\n",
    "    \n",
    "    Args:\n",
    "        url: Base API endpoint URL\n",
    "        \n",
    "    Returns:\n",
    "        List of results from all pages; or None (indicating no permission)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        separator = \"&\" if \"?\" in url else \"?\"\n",
    "        paginated_url = f\"{url}{separator}page={page}&per_page={PAGE_SIZE}\"\n",
    "        try:\n",
    "            response = requests.get(paginated_url, headers=HEADERS)\n",
    "            response.raise_for_status()\n",
    "            page_results = response.json()\n",
    "            if not page_results:\n",
    "                break\n",
    "                \n",
    "            results.extend(page_results)\n",
    "            page += 1\n",
    "            sleep(RATE_LIMIT_DELAY)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                status = e.response.status_code\n",
    "                if status == 404 and \"discussion_topics\" in url and \"only_announcements=true\" in url:\n",
    "                    return []\n",
    "                if status == 403:\n",
    "                    print(f\"Permission denied for URL: {paginated_url}\")\n",
    "                    return None\n",
    "            print(f\"Error fetching data from {paginated_url}: {str(e)}\")\n",
    "            break\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_course_data(course_id: int) -> Dict[str, Any] or None:\n",
    "    \"\"\"\n",
    "    Get all relevant data for a specific course.\n",
    "    If any resource returns no permission (None), stop crawling this course.\n",
    "    \n",
    "    Args:\n",
    "        course_id: Canvas course ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing course data; or None (indicating no permission)\n",
    "    \"\"\"\n",
    "    endpoints = {\n",
    "        'details': f\"{BASE_URL}/courses/{course_id}\",\n",
    "        'files': f\"{BASE_URL}/courses/{course_id}/files\",\n",
    "        'assignments': f\"{BASE_URL}/courses/{course_id}/assignments\",\n",
    "        'announcements': f\"{BASE_URL}/courses/{course_id}/discussion_topics?only_announcements=true\",\n",
    "        'users': f\"{BASE_URL}/courses/{course_id}/users\",\n",
    "        'quizzes': f\"{BASE_URL}/courses/{course_id}/quizzes\"\n",
    "    }\n",
    "    \n",
    "    course_data = {}\n",
    "    \n",
    "    # Get course details\n",
    "    try:\n",
    "        response = requests.get(endpoints['details'], headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        course_data['details'] = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching course details for course {course_id}: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    # Get other course resources\n",
    "    for resource, url in endpoints.items():\n",
    "        if resource != 'details':\n",
    "            data = get_paginated_results(url)\n",
    "            if data is None:\n",
    "                print(f\"Permission denied for resource '{resource}' in course {course_id}. Stopping crawl for this course.\")\n",
    "                return None\n",
    "            course_data[resource] = data\n",
    "            \n",
    "    return course_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_course_data(course_data: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Store course metadata in MongoDB and content chunks in ChromaDB\n",
    "    \n",
    "    Args:\n",
    "        course_data: Dictionary containing course details and resources\n",
    "        \n",
    "    Returns:\n",
    "        course_id: MongoDB ID of stored course\n",
    "    \"\"\"\n",
    "    # Store course details in MongoDB\n",
    "    course_id = course_collection.insert_one({\n",
    "        \"course_name\": course_data[\"details\"][\"name\"],\n",
    "        \"canvas_id\": course_data[\"details\"][\"id\"],\n",
    "        \"stored_at\": datetime.now()\n",
    "    }).inserted_id\n",
    "    \n",
    "    # Process and store different resource types\n",
    "    for resource_type in [\"files\", \"assignments\", \"announcements\"]:\n",
    "        if resource_type in course_data:\n",
    "            # Split content into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=50\n",
    "            )\n",
    "            \n",
    "            for item in course_data[resource_type]:\n",
    "                # Extract content based on resource type\n",
    "                content = item.get(\"description\", \"\") or item.get(\"body\", \"\") or item.get(\"content\", \"\")\n",
    "                \n",
    "                # Skip if content is empty\n",
    "                if not content.strip():\n",
    "                    continue\n",
    "                \n",
    "                chunks = text_splitter.create_documents(\n",
    "                    texts=[content],\n",
    "                    metadatas=[{\n",
    "                        \"course_id\": str(course_id),\n",
    "                        \"resource_type\": resource_type,\n",
    "                        \"title\": item.get(\"title\", \"\"),\n",
    "                        \"canvas_id\": item.get(\"id\", \"\")\n",
    "                    }]\n",
    "                )\n",
    "                \n",
    "                # Only process if we have chunks\n",
    "                if chunks:\n",
    "                    # Store chunks in ChromaDB\n",
    "                    chunk_ids = vector_db.add_documents(chunks)\n",
    "                    \n",
    "                    # Store metadata in MongoDB\n",
    "                    collection = globals()[f\"{resource_type[:-1]}_collection\"]\n",
    "                    collection.insert_one({\n",
    "                        \"course_id\": course_id,\n",
    "                        \"canvas_id\": item.get(\"id\", \"\"),\n",
    "                        \"title\": item.get(\"title\", \"\"),\n",
    "                        \"chunk_ids\": chunk_ids,\n",
    "                        \"stored_at\": datetime.now()\n",
    "                    })\n",
    "                \n",
    "    return str(course_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_knowledge_base(query: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search across course content using hybrid retrieval\n",
    "    \n",
    "    Args:\n",
    "        query: User query string\n",
    "        top_k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        results: List of relevant documents with metadata\n",
    "    \"\"\"\n",
    "    # Vector similarity search (MicroRAG)\n",
    "    vector_results = vector_db.similarity_search_with_score(\n",
    "        query,\n",
    "        k=top_k\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for doc, score in vector_results:\n",
    "        # Get associated metadata from MongoDB (MacroRAG)\n",
    "        metadata = doc.metadata\n",
    "        course = course_collection.find_one({\"_id\": metadata[\"course_id\"]})\n",
    "        \n",
    "        results.append({\n",
    "            \"content\": doc.page_content,\n",
    "            \"score\": score,\n",
    "            \"metadata\": {\n",
    "                \"course_name\": course[\"course_name\"],\n",
    "                \"resource_type\": metadata[\"resource_type\"],\n",
    "                \"title\": metadata[\"title\"]\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    # Log the query\n",
    "    query_log_collection.insert_one({\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"timestamp\": datetime.now()\n",
    "    })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_available_courses() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get all courses accessible by the current user\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/courses\"\n",
    "    courses = get_paginated_results(url)\n",
    "    \n",
    "    if courses is None:\n",
    "        print(\"Failed to get courses. Please check API key and permissions.\")\n",
    "        return []\n",
    "        \n",
    "    # Only keep active courses\n",
    "    active_courses = [\n",
    "        course for course in courses \n",
    "        if course.get('workflow_state') == 'available'\n",
    "    ]\n",
    "    \n",
    "    return active_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting available courses...\n",
      "Found 10 available courses\n",
      "\n",
      "Processing course: [PLP] Text Analytics (2025-02-10) (ID: 75454)\n",
      "Course 75454 stored with database ID: 67d53418ddc8ff85535092ba\n",
      "\n",
      "Processing course: EBA5004 Practical Language Processing [2420] (ID: 69955)\n",
      "Course 69955 stored with database ID: 67d5341dddc8ff85535092bf\n",
      "\n",
      "Processing course: IS06 MTech Internship (ID: 68113)\n",
      "Course 68113 stored with database ID: 67d53423ddc8ff85535092cb\n",
      "\n",
      "Processing course: ISY5004 ITSS GC Practice Module (Jan-May 2025) (ID: 74913)\n",
      "Error fetching data from https://canvas.nus.edu.sg/api/v1/courses/74913/quizzes?page=1&per_page=100: 404 Client Error: Not Found for url: https://canvas.nus.edu.sg/api/v1/courses/74913/quizzes?page=1&per_page=100\n",
      "Course 74913 stored with database ID: 67d53428ddc8ff85535092ce\n",
      "\n",
      "Processing course: MTech in EBAC/IS/SE (Thru-train) (ID: 27447)\n",
      "Course 27447 stored with database ID: 67d53438ddc8ff85535092d0\n",
      "\n",
      "Processing course: RC1000A A Culture of Respect and Consent (Student) (ID: 40630)\n",
      "Permission denied for URL: https://canvas.nus.edu.sg/api/v1/courses/40630/files?page=1&per_page=100\n",
      "Permission denied for resource 'files' in course 40630. Stopping crawl for this course.\n",
      "Failed to get data for course 40630\n",
      "\n",
      "Processing course: Real-time Audio-Visual Sensing and Sense Making (20-23Jan 2025) (ID: 74911)\n",
      "Course 74911 stored with database ID: 67d5343dddc8ff85535092d2\n",
      "\n",
      "Processing course: SE1000 Student Essentials (ID: 40629)\n",
      "Permission denied for URL: https://canvas.nus.edu.sg/api/v1/courses/40629/files?page=1&per_page=100\n",
      "Permission denied for resource 'files' in course 40629. Stopping crawl for this course.\n",
      "Failed to get data for course 40629\n",
      "\n",
      "Processing course: Spatial Reasoning from Sensor Data (13-15Jan 2025) (ID: 74909)\n",
      "Course 74909 stored with database ID: 67d53442ddc8ff85535092db\n",
      "\n",
      "Processing course: Vision Systems (6-10Jan 2025) (ID: 74907)\n",
      "Course 74907 stored with database ID: 67d53447ddc8ff85535092e1\n",
      "\n",
      "Total courses stored: 8\n",
      "Course 75454: [PLP] Text Analytics (2025-02-10)\n",
      "Course 69955: EBA5004 Practical Language Processing [2420]\n",
      "Course 68113: IS06 MTech Internship\n",
      "Course 74913: ISY5004 ITSS GC Practice Module (Jan-May 2025)\n",
      "Course 27447: MTech in EBAC/IS/SE (Thru-train)\n",
      "Course 74911: Real-time Audio-Visual Sensing and Sense Making (20-23Jan 2025)\n",
      "Course 74909: Spatial Reasoning from Sensor Data (13-15Jan 2025)\n",
      "Course 74907: Vision Systems (6-10Jan 2025)\n"
     ]
    }
   ],
   "source": [
    "# Get all course IDs from the system\n",
    "print(\"Getting available courses...\")\n",
    "available_courses = get_all_available_courses()\n",
    "all_courses_data = {}\n",
    "\n",
    "if not available_courses:\n",
    "    print(\"No courses found or unable to access courses.\")\n",
    "else:\n",
    "    print(f\"Found {len(available_courses)} available courses\")\n",
    "    \n",
    "    # Get and store data for each course\n",
    "    for course in available_courses:\n",
    "        course_id = course['id']\n",
    "        print(f\"\\nProcessing course: {course['name']} (ID: {course_id})\")\n",
    "        \n",
    "        course_data = get_course_data(course_id)\n",
    "        if course_data:\n",
    "            all_courses_data[course_id] = course_data\n",
    "            stored_id = store_course_data(course_data)\n",
    "            print(f\"Course {course_id} stored with database ID: {stored_id}\")\n",
    "        else:\n",
    "            print(f\"Failed to get data for course {course_id}\")\n",
    "\n",
    "# Print storage statistics\n",
    "print(f\"\\nTotal courses stored: {len(all_courses_data)}\")\n",
    "for course_id, data in all_courses_data.items():\n",
    "    print(f\"Course {course_id}: {data['details']['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stored Course Data:\n",
      "\n",
      "Course ID: 67d53151ddc8ff855350929f\n",
      "Course Name: [PLP] Text Analytics (2025-02-10)\n",
      "\n",
      "Stored Document Chunks:\n",
      "\n",
      "Document Content: <p>This quiz is part of your Mini Project. Explore the accident reports data given in osha.txt, and answer the questions here.</p><script src=\"https://instructure-uploads-apse1.s3.ap-southeast-1.amazo...\n",
      "\n",
      "Document Content: <p>Please use the training dataset <a id=\"5746888\" class=\"instructure_file_link inline_disabled\" title=\"Link\" href=\"https://canvas.nus.edu.sg/courses/75454/files/5746888?verifier=zQYjUUhRE3yQa7rh9cgLt...\n",
      "\n",
      "Document Content: and fine-tune your classifier with any step necessary.</p>...\n",
      "\n",
      "Document Content: <p>Then test your classifier on top of <a id=\"5746884\" class=\"instructure_file_link inline_disabled\" title=\"Link\" href=\"https://canvas.nus.edu.sg/courses/75454/files/5746884?verifier=t2RHd0GWpNF7tk67H...\n",
      "\n",
      "Document Content: <p>Also recommend to try out API with LLM based classifiers to compare the result.&nbsp;</p>\n",
      "<p>&nbsp;</p><script src=\"https://instructure-uploads-apse1.s3.ap-southeast-1.amazonaws.com/account_2145000...\n",
      "\n",
      "Document Content: <p>Given the&nbsp; <a id=\"5912767\" class=\"instructure_file_link inline_disabled\" title=\"Link\" href=\"https://canvas.nus.edu.sg/courses/75454/files/5912767?verifier=JGwrPtqQQFXMysrE7bH3IFpYfyTYvYkDuq9SH...\n",
      "\n",
      "Document Content: <p>Question: What are the major features (Top 3) mentioned and cared most by the reviewers?</p>\n",
      "<p>&nbsp;</p>\n",
      "<p>&nbsp;&nbsp;</p>\n",
      "<p>&nbsp;</p><script src=\"https://instructure-uploads-apse1.s3.ap-sout...\n",
      "\n",
      "Document Content: <p>Please submit the following file for your mini-project assignment:</p>\n",
      "<ol style=\"list-style-type: decimal;\">\n",
      "<li>Option 1 - A word document (file name \"yourname-mini.docx\") summarizing your findin...\n",
      "\n",
      "Document Content: </ol><script src=\"https://instructure-uploads-apse1.s3.ap-southeast-1.amazonaws.com/account_214500000000000001/attachments/18442/BlueCanvasMobileConfig.js\"></script>...\n",
      "\n",
      "Vector Store Stats:\n",
      "Total documents: 9\n"
     ]
    }
   ],
   "source": [
    "# Query MongoDB to check stored data\n",
    "print(\"\\nStored Course Data:\")\n",
    "for course in course_collection.find():\n",
    "    print(f\"\\nCourse ID: {course['_id']}\")\n",
    "    print(f\"Course Name: {course['course_name']}\")\n",
    "    \n",
    "print(\"\\nStored Document Chunks:\")\n",
    "for doc in vector_db.get()[\"documents\"]:\n",
    "    print(f\"\\nDocument Content: {doc[:200]}...\")\n",
    "\n",
    "print(\"\\nVector Store Stats:\")\n",
    "print(f\"Total documents: {vector_db._collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Test query\n",
    "results = query_knowledge_base(\"When is the project submission deadline?\")\n",
    "for result in results:\n",
    "    print(f\"\\nScore: {result['score']}\")\n",
    "    print(f\"Course: {result['metadata']['course_name']}\")\n",
    "    print(f\"Content: {result['content'][:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
